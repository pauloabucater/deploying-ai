{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391397a5",
   "metadata": {},
   "source": [
    "# LM Studio\n",
    "\n",
    " LM Studio is a powerful platform designed for working with large language models (LLMs) on your local machine. It provides an intuitive interface for downloading, managing, and running a wide variety of open-source LLMs.\n",
    "\n",
    "A few characteristics of LM Studio are:\n",
    "\n",
    "+ With LM Studio, users can experiment with different models, customize their settings, and interact with them in real time.\n",
    "+ The platform supports both CPU and GPU acceleration, making it accessible for a range of hardware configurations.\n",
    "+ LM Studio is ideal for developers, researchers, and enthusiasts who want to explore the capabilities of LLMs without relying on cloud-based solutions.\n",
    "\n",
    "You can download and install LMStudio from [lmstudio.ai](https://lmstudio.ai/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e751e18",
   "metadata": {},
   "source": [
    "## LM Studio API\n",
    "\n",
    "An important characteristic in LM Studio is its support for OpenAI's API:\n",
    "\n",
    "+ LM Studio accepts requests on several OpenAI endpoints and returns OpenAI-like response objects ([lmstudio.ai](https://platform.openai.com/docs/guides/prompt-engineering#message-roles-and-instruction-following)).\n",
    "+ This means that we can reuse our code by pointing the local API using the `base_url` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc96b0b",
   "metadata": {},
   "source": [
    "## Setup LM Studio\n",
    "\n",
    "+ Download and install LM Studio from [lmstudio.ai](https://lmstudio.ai/).\n",
    "+ Start the application.\n",
    "+ Select a model to load:\n",
    "\n",
    "    - From the developer tab, you can select a model from the top bar.\n",
    "    - Select a model, for example, `qwen/qwen3-4b-2507`.\n",
    "    - Start the server with the control on the top-left.\n",
    "\n",
    "![](./img/01_lmstudio.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3a04b",
   "metadata": {},
   "source": [
    "With the setup above, we can simply point to the local server as indicated below. The API works similarly than with OpenAI, however, the underlynig model is running locally and it was not necessarily produced by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93f5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model = 'local-model',\n",
    "    input = 'Hello world!',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d1a1fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nHello! ðŸ˜Š How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

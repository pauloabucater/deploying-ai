{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac9fa3b",
   "metadata": {},
   "source": [
    "# Representing Text\n",
    "\n",
    "We can represent text in many ways: character strings are a standard representation, but we can also create numerical representations of text. In this notebook we will discuss embeddings.\n",
    "\n",
    "## Features\n",
    "\n",
    "Features to any (machine learning) model can be continuous or categorical.\n",
    "\n",
    "- We use continuous features to represent numerical values: income, number of times the user clicked on a link, prices, etc.\n",
    "- Categorical features represent an instance of a class or category. They have a finite number of possible values: job title, genre of a movie, breed of a dog, etc.\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "An **embedding** is a trained numerical representation of a categorical feature:\n",
    "\n",
    "- We use the word *trained* to highlight that embeddings are learned during model training.\n",
    "- Different models and training procedures can be used to obtained embeddings. Word2Vec and BERT embeddings, for example, are different and capture different characteristics of the features.\n",
    "\n",
    "[OpenAI's documentation](https://platform.openai.com/docs/guides/embeddings) include a few uses of embeddings:\n",
    "\n",
    "\n",
    "- Search: results are ranked by relevance to a query string.\n",
    "- Clustering:  text strings are grouped by similarity.\n",
    "- Recommendations:  items with related text strings are recommended.\n",
    "- Anomaly detection:  outliers with little relatedness are identified.\n",
    "- Diversity measurement: similarity distributions are analyze.\n",
    "- Classification: text strings are classified by their most similar label.\n",
    "\n",
    "## BERT Embeddings\n",
    "\n",
    "Bert embeddings are computed based on three ingredients discussed below:\n",
    "\n",
    "+ Token embeddings (also called word embeddings)\n",
    "+ Positional embeddings\n",
    "+ Token type embeddings (also called sentence embeddings)\n",
    "\n",
    "![](img/02_bert_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f396f4",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Embedding computations start with tokenization: representing the original text as tokens in a vocabulary. \n",
    "\n",
    "To illustrate the process, we can use the [`transformers`](https://huggingface.co/docs/transformers/en/index) library from [HuggingFace](https://huggingface.co/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0957363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5014d73b644470bcf0211fe98c158e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\pabucater\\Documents\\DSI-DeployingAI1\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\pabucater\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d3b0c6265d412f834741740c4eb795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8e4576757b4505bf492cb02ef72a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149b8c3f4c00410b9182edd4fd0bb3de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 8870, 2024, 4569, 102]], 'token_type_ids': [[0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "documents = [\"cats are fun\"]\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased')\n",
    "tokens = tokenizer(documents)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e29323",
   "metadata": {},
   "source": [
    "In the code snippet above, we used the `transformers` library to obtain the tokens that represent the phrase 'cats are fun'. The tokenizer returns a dictionary with an entry called `'input_ids'`, which contains an array of four integers. These integers are the positions of each token in the model's vocabulary. The vocabulary is [`'bert-base-uncased'`](https://huggingface.co/bert-base-uncased/blob/main/vocab.txt) and it is applied using the method `.from_pretrained()`.\n",
    "\n",
    "We can show the vocabulary entries with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70b566b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of 'cats': 8870\n",
      "Index of 'are': 2024\n",
      "Index of 'fun': 4569\n"
     ]
    }
   ],
   "source": [
    "print(f\"Index of 'cats': {tokenizer.vocab['cats']}\")\n",
    "print(f\"Index of 'are': {tokenizer.vocab['are']}\")\n",
    "print(f\"Index of 'fun': {tokenizer.vocab['fun']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2161fa",
   "metadata": {},
   "source": [
    "IDs 101 and 102 are special tokens:\n",
    "\n",
    "- ID 101 is the `[CLS]` token, indicating the begginning of a sequence.\n",
    "- ID 102 is the `[SEP]` token, indicating the end of a sequence.\n",
    "\n",
    "They are inserted automatically to the output of the BERT tokenizer. The BERT Tokenizer includes 30,522 unique tokens. In addition, the BERT tokenizer handles unkown tokens, `[UNK]`, using techniques such as WordPiece. You can read more about this tokenizer in [BERT Tokenization (Nowak, 2023)](https://tinkerd.net/blog/machine-learning/bert-tokenization/) and Mastering Text Similarity ([Guadagnolo, 2024](https://medium.com/eni-digitalks/mastering-text-similarity-combining-embedding-techniques-and-distance-metrics-98d3bb80b1b6)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8c5e5",
   "metadata": {},
   "source": [
    "### Token Embeddings\n",
    "\n",
    "The tokens obtained from the previous step are mapped to the model's precomputed embeddings. For each token in the model vocabulary, there is an embedding vector.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/02_skip_gram_architecture.png\" width=\"700\"/>\n",
    "</div>\n",
    "\n",
    "Image source: Mastering Text Similarity ([Guadagnolo, 2024](https://medium.com/eni-digitalks/mastering-text-similarity-combining-embedding-techniques-and-distance-metrics-98d3bb80b1b6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "embedding_layer = model.embeddings\n",
    "embedding_layer.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377eac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer.word_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bdc2f8",
   "metadata": {},
   "source": [
    "The attribute `.weight` of the embedding layer shows the actual embeddings. It is a matrix of 30,522 rows and 768 columns (the object, in reality, is a 2-dimenional vector). \n",
    "\n",
    "+ The number of rows is equal to the size of the model vocabulary.\n",
    "+ The number of columns is the hidden size or the size of the model's internal representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb201c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(['cats are fun'])\n",
    "input_ids = tokens.input_ids[0]\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee568e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = embedding_layer.word_embeddings.weight[input_ids]\n",
    "doc_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245f57b",
   "metadata": {},
   "source": [
    "### Position Embeddings\n",
    "\n",
    "In addition to token embeddings, the BERT model also keeps track of positions through position embeddings. In contrast with token embeddings, position embeddings have shape (512, 768). This is because the BERT model can only take up to 512 tokens at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c93640",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer.position_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face8a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer.position_embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8877b28",
   "metadata": {},
   "source": [
    "### Token Type Embeddings\n",
    "\n",
    "Token or Segment Type embeddings. BERT was trained to solve Next Sentence Prediciton. Given two sentences, A and B, BERT was trained to determine if B logically follows A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer.token_type_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68852cee",
   "metadata": {},
   "source": [
    "### The Embedding Layer\n",
    "\n",
    "The embedding layer takes a list of token ids and converts them to embeddings that combine the three types discussed above: token, position, and type embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b424e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tokens = tokenizer(['cats are fun'])\n",
    "final_embeddings = embedding_layer(input_ids = torch.tensor(tokens.input_ids))\n",
    "final_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
